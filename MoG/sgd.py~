"""
Stochastic Gradient Descent on Mixture of Gaussians

"""

import numpy as np
import scipy
import theano
import theano.tensor as T
import theano.sandbox.linalg.ops
import theano.tensor.nnet
import theano.printing
import math
import pylab
from pylab import *
from matplotlib.figure import Figure
import time
import random

inv = theano.sandbox.linalg.ops.MatrixInverse()
det = theano.sandbox.linalg.ops.Det()
diag = theano.sandbox.linalg.ops.ExtractDiag()

theano.config.compute_test_value = 'off'

class MoG(object):
    """ Mixture of Gaussians
    Defined by :
    - the number of Gaussians k
    - mean vectors for each of these Gaussians
    - covariance matrices for each of these Gaussians

    Depends on :
    -dimension of the inputs dim
    """

    def __init__(self, inputs, N, dim, k, muBase):
        """ Initialize the Gaussian Mixture Model
        :type inputs: theano.tensor.TensorType
        :param inputs: symbolic variable that describes the inputs
        
        :type dim: int
        :param dim: dimensionality of the input space

        :type k: int
        :param k: number of Gaussians in the model

        """


        # number of gaussians used in the model
        self.NoG = k
        

        #dimension of the input examples
        self.dimOfInput = dim

        #number of training examples
        self.Ninput = N
        

        #the means are initialized randomly  
        #random.seed()        
        self.mu =theano.shared(value=np.random.rand(k, dim), name = 'mu')

        

        #the covariance matrices are defined as U.T * U, U, being a square matrix, to ensure the covariance matrices to always be positive semi-definite
        id = np.eye(dim) 
        a = id
        for i in range(k-1):
            a = np.concatenate((a, id))
        self.U = theano.shared(value=a.reshape((k, dim, dim)), name = 'U')

        #the repartition probabilities are defined as softmax of a random vector v
        self.v = theano.shared(value=np.random.rand(k),
                                name=('v'))


    def getSigma(self, index):
        return np.dot((self.U.get_value())[index].T, self.U.get_value()[index])

    def getPi(self):
        return T.nnet.softmax(self.v)

    def normalPdf(self, inputs):
        """Return the values of the probability density function given for inputs' points

        """
        
        def oneGaussian(index, inputs):

            mu = self.mu[index]
            UTemp = self.U[index]
            SigmaTemp = T.dot(UTemp.T, UTemp)
            d = inputs - mu.T
            SigmaInv =  inv(SigmaTemp)
            #SigmaInv =theano.printing.Print('SigmaInv')(SigmaInv)
            expArg1 = T.dot(d, SigmaInv)
            expArg2 = T.dot(expArg1, d.T)
            expArgDiag = diag(expArg2)
            #expArgDiag = theano.printing.Print(' expArgDiag')(expArgDiag)
            e = T.exp(-0.5 * expArgDiag)
            #e = theano.printing.Print('e')(e)
            detInt = 2 * math.pi * SigmaTemp
            inSqrt = det(detInt)
            factor = inSqrt**(-0.5)
            #factor = theano.printing.Print('factor')(factor)
            N = e * factor
            return N
        
 
        MoG, _ = theano.map(fn = oneGaussian,
                            sequences = [T.arange(self.NoG)],
                            non_sequences = inputs)
        
        
        return (MoG)
    
    def logNormalPdf(self, inputs):
        """Return the values of the probability density function given for inputs' points

        """
        
        def logOneGaussian(index, inputs):

            mu = self.mu[index]
            UTemp = self.U[index]
            SigmaTemp = T.dot(UTemp.T, UTemp)
            d = inputs - mu.T
            SigmaInv =  inv(SigmaTemp)
            #SigmaInv =theano.printing.Print('SigmaInv')(SigmaInv)
            term1Arg = T.dot(T.dot(d, SigmaInv), d.T)
            term1 = diag(term1Arg)
            detArg = 2 * math.pi * SigmaTemp
            logArg= det(detArg)
            term2 = T.log(logArg)
            N = -0.5 * (term1 + term2)
            return N
        
 
        logMoG, _ = theano.map(fn = logOneGaussian,
                            sequences = [T.arange(self.NoG)],
                            non_sequences = inputs)
        
        
        return (logMoG)
    
    
    def neg_log_likelihood(self, inputs):
        """ Return the log-likelihood of this model given the input data

        """
        return   -(T.sum(T.log(T.sum(T.nnet.softmax(self.v) * self.normalPdf(inputs).T, axis = 1))))
       


  

    

def gradientDescent(inputs, N, dim, numberOfGaussians, NiterMax,  learning_rate, batch_size):
    """Run a stochastic gradient descent to find an optimal value for the mixture of Gaussians parameters
    """


    inputs_shared = theano.shared(value = inputs, name = "inputs")

    #building the model
    batch_index = T.lscalar()    # index to a [mini]batch 
    X = T.dmatrix('X')     #input data
    

    n_minibatch = N / batch_size  #number of minibatches
    
    model = MoG(X, N, dim, numberOfGaussians, mean)
    cost = model.neg_log_likelihood(X)

    # compute the gradient of cost with respect to mu, U, and v
    g_mu = T.grad(cost = cost, wrt = model.mu)
    g_U = T.grad(cost = cost, wrt = model.U)
    g_v = T.grad(cost = cost, wrt = model.v)

    # specify how to update the parameters of the model as a dictionary
    updates ={model.mu: model.mu - learning_rate*g_mu,\
              model.U: model.U - learning_rate*g_U,\
              model.v: model.v - learning_rate*g_v}
  
    
    train_model = theano.function(inputs=[batch_index], \
                                  outputs = cost,\
                                  updates = updates, \
                                  givens={
                                      X: inputs_shared[batch_index*batch_size:(batch_index+1)*batch_size]})

    #compute responsabilities for visualization
    ## ###
    ## # without logsumexp trick
    ## ###
    ## numerator_responsabilities = (T.nnet.softmax(model.v)) * model.normalPdf(X).T
    ## denominator_responsabilities =  T.sum(numerator_responsabilities, axis = 1) #+ (1e-2)
    ## responsabilities  = (numerator_responsabilities.T / denominator_responsabilities).T

    ###
    # with logsumexp trick
    ###
    logPi = T.log(T.nnet.softmax(model.v))
    z_max = T.max(model.logNormalPdf(X).T + logPi, axis = 1)
    expArg = (model.logNormalPdf(X).T + logPi).T - z_max
    e = T.exp(expArg)
    logArg = T.sum(e, axis = 0)
    sumPi = z_max +  T.log(logArg)
    logResp = ((model.logNormalPdf(X).T + logPi).T - sumPi).T
    responsabilities = T.exp(logResp)
        
  
    computeResponsabilities = theano.function([X], responsabilities)

    PiToPrint = model.getPi()

    printPi = theano.function([], PiToPrint)
    
    
    
    print("initial values ---")
    print("means")
    print(model.mu.get_value())
    print("covariance matrices")
    for index in range(model.NoG):
        print(model.getSigma(index))
    print("Mixing coefficients")
    print(printPi())
    print("---")
    scatter(inputs[:,0], inputs[:,1])
    show()


    t = 0
    done = False
    logLikelihood = 0
    logLikTab = np.zeros(n_minibatch)
    start = time.clock()
    
    while((t<NiterMax) and not(done)):
       
        oldLogLik = logLikelihood
        
        for minibatch_index in range(n_minibatch):        
            logLikTab[minibatch_index]=  train_model(minibatch_index)
            
        logLikelihood = logLikTab.sum()
        computedResponsabilities = computeResponsabilities(inputs)

        
        if (done):
            clf()
            scatter(inputs[:,0], inputs[:,1], c = computedResponsabilities )
            draw()
            print("logLikelihood")
            print(logLikelihood)
            print("responsabilities")
            print(computedResponsabilities.sum(axis = 0))
            print("means")
            print(model.mu.get_value())
            print("covariance matrices")
            for index in range(model.NoG):
                print(model.getSigma(index))
            print("Mixing coefficients")
            print(printPi())

        

        with open('./sgd', 'a') as f:
            f.write(str(logLikelihood))
            f.write("\n")
            f.write(str(time.clock()- start))
            f.write("\n")

        t = t+1
        done =  (round(oldLogLik, 3) == round(logLikelihood, 3))

    scatter(inputs[:,0], inputs[:,1], c = computedResponsabilities )
    draw()
    print("logLikelihood")
    print(logLikelihood)
    print("responsabilities")
    print(computedResponsabilities.sum(axis = 0))
    print("means")
    print(model.mu.get_value())
    print("covariance matrices")
    for index in range(model.NoG):
        print(model.getSigma(index))
    print("Mixing coefficients")
    print(printPi())
        
    return model



def graphicalInterface(inputs, model):
    scatter(inputs[:,0], inputs[:,1])
    #mixtureOfGaussians = T.sum(model.normalPdf(inputs).T * model.pi, axis = 0)
    
    #contour(inputs[:,0], inputs[:,1], mixtureOfGaussians)
    #imshow( scipy.log(mixtureOfGaussians + mixtureOfGaussians.min()))
    show()
    


def dataset(N):
    ## X = np.random.randn(100, 2) * 0.2 
    ## Y = np.random.randn(100, 2) * 0.4  + np.array([1, 3]) 
    ## Z = np.random.randn(100, 2) * 0.4   - 1.5
    ## A = np.random.randn(100, 2) * 0.2 + np.array([1, 3]) 
    ## B = np.random.randn(100, 2) * 0.1  + np.array([2.5, 1.2])
    ## C = np.random.randn(100, 2) * 0.5  +  np.array([1.5, -3])

    ## ## res1 = np.concatenate((X, np.concatenate((Y, Z))))
    ## ## res2 = np.concatenate((A, np.concatenate((B, C))))
    ## ## res = np.concatenate((res1, res2))

    ## #res = np.concatenate((Y, Z))

    ## res =  np.concatenate((X, np.concatenate((Y, Z))))

    res = np.empty((N, 2))

    for i in range(N):
        k = random.randint(1,3)
        if k == 1:
            res[i] =  np.random.randn(2) * 0.2
        elif k == 2:
            res[i] = np.random.randn(2) * 0.4  + np.array([1, 3])
        else:
            res[i] = np.random.randn(2) * 0.4   - 1.5
        
    
    return res




def run():
    #creation of the input set
    inputs = dataset(300)

    #clearing log file
    with open('./sgd', 'w',) as f:
        f.write('')
    
    #initialization of parameters
    N, dim = inputs.shape
    k = 3
    iterMax = 5000
    alpha = 0.0001
    batch_size = 60
    
    model = gradientDescent(inputs, N, dim, k, iterMax, alpha, batch_size)
    #graphicalInterface(inputs, model)
    

    
    

    
if __name__ == '__main__':
    run()
